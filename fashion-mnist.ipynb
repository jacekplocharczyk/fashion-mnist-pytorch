{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground with the fashion-mnist\n",
    "\n",
    "## Features explanation\n",
    "Each column represent pixel gray scale from 0 to 255 (8 bit).\n",
    "\n",
    "## Lables explanation\n",
    "| Label | Description |\n",
    "| --- | ---- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading fashionmnist.zip to data\n",
      "\n",
      "Archive:  data/fashionmnist.zip\n",
      "  inflating: data/t10k-images-idx3-ubyte  \n",
      "  inflating: data/fashion-mnist_test.csv  \n",
      "  inflating: data/train-labels-idx1-ubyte  \n",
      "  inflating: data/train-images-idx3-ubyte  \n",
      "  inflating: data/fashion-mnist_train.csv  \n",
      "  inflating: data/t10k-labels-idx1-ubyte  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68.8M/68.8M [00:03<00:00, 18.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -frd /data\n",
    "kaggle datasets download -d zalando-research/fashionmnist -p data\n",
    "unzip data/fashionmnist.zip -d data/\n",
    "rm data/t*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set data split and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_import = 5000 # how many rows to import (1 - 60000), if None use all data\n",
    "\n",
    "train = 0.70\n",
    "test = 0.15\n",
    "cv = 1 - train - test\n",
    "\n",
    "split = (train, test, cv)\n",
    "assert cv > 0 and cv < 1, f\"cross validation share has to be between 0 and 1 not {cv}\"\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMnist(Dataset):\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_csv_length(csv: Path) -> int:\n",
    "        \"Get rows count of a csv file\"\n",
    "        with open(csv) as f:\n",
    "            return sum(1 for line in f)\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_split(split: Tuple[int, int, int]) -> None:\n",
    "        \"Check if split elements are positive and sum to one\"\n",
    "        sum_ = 0\n",
    "        for i, element in enumerate(split):\n",
    "            assert element >= 0, f\"Split element {i} is negative: {element}\"            \n",
    "            assert element <= 1, f\"Split element {i} is greater than 1: {element}\"\n",
    "            sum_ += abs(element)\n",
    "        assert sum_ == 1, f\"Sum of a split elements is unequal to 1\"\n",
    "    \n",
    "    def _read_csv(self, \n",
    "                  test: bool,\n",
    "                  cv: bool,\n",
    "                  rows_no: Union[None, int], \n",
    "                  split: Tuple[int, int, int]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read csv file to get specific dataset (train, test or cv)\n",
    "        :param test: True if it is a test set\n",
    "        :param cv: True if it is a cross-validation set\n",
    "        :param rows_no: number of rows to use (None == all rows)\n",
    "        :param split: tuple with train, test, and cv shares, e.g. (0.8, 0.1, 0.1)\n",
    "        :return: pd.DataFrame with rows and header\n",
    "\n",
    "        \"\"\"\n",
    "        rows_in_csv = self._get_csv_length(self.csv_path)\n",
    "        \n",
    "        if rows_no is None:\n",
    "            rows_no = rows_in_csv\n",
    "\n",
    "        assert (rows_no >= 1) and (rows_no <= rows_in_csv), f\"rows_no is exceeding 1 - {rows_in_csv} range: {rows_no}\"\n",
    "        assert not (test and cv), \"Dataset cannot be a test and a cross-validation at the same moment\"\n",
    "        self._check_split(split)\n",
    "        \n",
    "        train_no = int(rows_no * split[0])\n",
    "        test_no =  int(rows_no * split[1])\n",
    "        cv_no =  int(rows_no * split[2])\n",
    "\n",
    "        cv_boundary = train_no + test_no\n",
    "        \n",
    "        if test:\n",
    "            df = pd.read_csv(self.csv_path, skiprows=range(1, train_no), nrows=test_no)\n",
    "        elif cv:\n",
    "            df = pd.read_csv(self.csv_path, skiprows=range(1, cv_boundary), nrows=cv_no)\n",
    "        else:\n",
    "            df = pd.read_csv(self.csv_path, nrows=train_no)\n",
    "        \n",
    "        return df        \n",
    "    \n",
    "    def __init__(self, \n",
    "                 transform=None, \n",
    "                 test: bool = False,\n",
    "                 cv: bool = False,\n",
    "                 rows_no: Union[None, int] = None, \n",
    "                 split: Tuple[int, int, int] = (1, 0, 0)):\n",
    "        \"\"\"\n",
    "        Create Dataset containing the Fashon Mnist data.\n",
    "        :param transform: transformations to perform on the dataset\n",
    "        :param test: True if it is a test set\n",
    "        :param cv: True if it is a cross-validation set\n",
    "        :param rows_no: number of rows to use (None == all rows)\n",
    "        :param split: tuple with train, test, and cv shares, e.g. (0.8, 0.1, 0.1)\n",
    "\n",
    "        \"\"\"\n",
    "        self.csv_path = Path('data/fashion-mnist_train.csv')\n",
    "        data = self._read_csv(test, cv, rows_no, split)\n",
    "        self.data = data.to_numpy()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "            \n",
    "        image = self.data[index, 1:]\n",
    "        label = self.data[index, 0]\n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return ConcatDataset([self, other])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        return {'image': torch.from_numpy(image).float(),\n",
    "                'label': torch.tensor(label).long()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transform=transforms.Compose([ToTensor()])\n",
    "\n",
    "train_dataset = FashionMnist(rows_no=rows_to_import, transform=transform, split=split)\n",
    "test_dataset = FashionMnist(rows_no=rows_to_import, transform=transform, split=split, test=True)\n",
    "cv_dataset = FashionMnist(rows_no=rows_to_import, transform=transform, split=split, cv=True)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,  shuffle=True, num_workers=num_workers)\n",
    "cv_dataloader = torch.utils.data.DataLoader(cv_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.softmax(self.fc4(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Optimizer and the Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_no = 4\n",
    "train_stats_freq = 1  # calculate stats every x epoch\n",
    "cv_stats_freq = 2  # calculate cv stats every x epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stats:\n",
    "    @property\n",
    "    def loss(self) -> Tuple[np.array, np.array]:\n",
    "        \"Get epochs and loss\"\n",
    "        return self.epochs, self.losses\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self) -> Tuple[np.array, np.array]:\n",
    "        \"Get epochs and accuracy\"\n",
    "        return self.epochs, self.accuracies\n",
    "\n",
    "    \n",
    "class TrainStats(Stats):\n",
    "    def __init__(self, stats_freq: int):\n",
    "        \"\"\"\n",
    "        Calculate and show stats related to the train data.\n",
    "        :param stats_freq: print stats every stats_freq batch\n",
    "        \"\"\"\n",
    "        self.stats_freq = stats_freq\n",
    "        self.accuracies = np.array([], dtype=float)\n",
    "        self.losses = np.array([], dtype=float)\n",
    "        self.epochs = np.array([], dtype=int)\n",
    "        \n",
    "        \n",
    "    def __call__(self, epoch: int, loss: float, accuracy: float, *args, **kwargs):\n",
    "        if epoch % self.stats_freq == self.stats_freq - 1:\n",
    "            self.accuracies = np.append(self.accuracies, accuracy)\n",
    "            self.losses = np.append(self.losses, loss)\n",
    "            self.epochs = np.append(self.epochs, epoch + 1)\n",
    "    \n",
    "    \n",
    "class CVStats(Stats):\n",
    "    def __init__(self, stats_freq: int, cv_dataloader):\n",
    "        \"\"\"\n",
    "        Calculate and show stats related to the cross-validation data.\n",
    "        :param stats_freq: print stats every stats_freq batch\n",
    "        :param cv_dataloader: dataloader with the cv data\n",
    "        \"\"\"\n",
    "        self.stats_freq = stats_freq\n",
    "        self.accuracies = np.array([], dtype=float)\n",
    "        self.losses = np.array([], dtype=float)\n",
    "        self.epochs = np.array([], dtype=int)\n",
    "        self.cv_dataloader = cv_dataloader\n",
    "        \n",
    "    def __call__(self, model):\n",
    "        if epoch % self.stats_freq == self.stats_freq - 1:\n",
    "            loss, accuracy = self._get_cv_loss_and_accuracy(model)\n",
    "            self.accuracies = np.append(self.accuracies, accuracy)\n",
    "            self.losses = np.append(self.losses, loss)\n",
    "            self.epochs = np.append(self.epochs, epoch + 1)\n",
    "\n",
    "\n",
    "    def _get_cv_loss_and_accuracy(self, model) -> Tuple[float, float]:\n",
    "        \"Compute CV loss on the whole CV dataset\"\n",
    "        with torch.no_grad():\n",
    "            accumualted_loss = 0.0\n",
    "            epoch_good_predicitions = 0\n",
    "\n",
    "            for i, data in enumerate(cv_dataloader, 0):\n",
    "                inputs, labels = data['image'], data['label']\n",
    "\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                batch_size = outputs.shape[0]\n",
    "                accumualted_loss += loss.item() * batch_size\n",
    "                max_index = outputs.max(dim = 1)[1]\n",
    "                epoch_good_predicitions += int((max_index == labels).sum())\n",
    "                \n",
    "        accuracy = epoch_good_predicitions / (cv * rows_to_import)\n",
    "        mean_loss = accumualted_loss / (cv * rows_to_import)\n",
    "\n",
    "        return mean_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "train_stats = TrainStats(train_stats_freq)\n",
    "cv_stats = CVStats(cv_stats_freq, cv_dataloader)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(epoch_no):  # loop over the dataset multiple times\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    accumualted_loss = 0.0\n",
    "    epoch_good_predicitions = 0\n",
    "    \n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data['image'], data['label']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = outputs.shape[0]\n",
    "\n",
    "        accumualted_loss += loss.item() * batch_size\n",
    "        max_index = outputs.max(dim = 1)[1]\n",
    "        epoch_good_predicitions += int((max_index == labels).sum())\n",
    "\n",
    "    epoch_accuracy = epoch_good_predicitions / (train * rows_to_import)\n",
    "    mean_loss = accumualted_loss / (train * rows_to_import)\n",
    "    train_stats(epoch, mean_loss, epoch_accuracy)\n",
    "    cv_stats(net)\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
